# RAG (RETRIEVAL AUGMENTED GENERATION) Chatbot System for e-commerce

Retrieval-Augmented Generation (RAG) is a hybrid AI approach that combines retrieval-based and generation-based techniques to enhance the quality of responses in a language model. 

## Overview

This project combines **ChromaDB** for storing and retrieving document embeddings and **Ollama** for generating responses based on those embeddings. The chatbot can be queried via an API, and it retrieves relevant chunks of data to provide context-aware and up-to-date responses.


## Workflow

### 1. **User Query**
   - The user submits a query through the API endpoint ( `/chat`).
   - This query can be about product information.

### 2. **Query Processing**
   - The system receives the query in the backend as an HTTP POST request.
   - The query is extracted from the request body and prepared for processing.

### 3. **Document Retrieval**
   - The system retrieves relevant documents based on the user's query.
   - The query is processed using a **retrieval model** (such as embeddings generated by Ollama).
   - The system searches for relevant documents stored in a **vector database** (ChromaDB), which contains pre-embedded data such as product descriptions, articles, or knowledge bases.

### 4. **Embedding Generation**
   - The retrieved documents are embedded using a pre-trained embedding model (`nomic-embed-text`).
   - This model converts the documents into vector representations, which are used for semantic comparison.

### 5. **Response Generation**
   - The query and the relevant documents are passed to a **language generation model** (`gemma:2b`).
   - This model generates a human-like response based on the context of the query and the retrieved documents.
   - The response is contextually accurate and tailored to the user's question.



## Key Features

- **Contextual Search**: Finds the most relevant documents based on the userâ€™s query.
- **Embeddings**: Converts text documents into vector representations for fast similarity-based search.
- **Natural Language Generation**: Uses a large language model to craft human-like responses based on context.



## Technologies Used

- **Node.js**: Backend for handling queries and responses.
- **ChromaDB**: Vector database for storing and retrieving embedded documents.
- **Ollama**: Embedding generation and LLM model for both retrieval and generation tasks.

---

## Prerequisites
The following softwares must exist on the local machine to be able to run the rag chatbot:
- ChromaDB
- Ollama
- Nodejs

## Getting Started

1. Clone the repository:
   ```bash
   git clone https://github.com/0xaplus/rag-chatbot.git
   cd rag-chatbot

2. Install dependencies:
```bash
$ npm install
```

3. Populate ChromaDB with datasets
```bash
$ tsx import.ts
```

4. Run server
```bash
$ tsx index.ts


Server is running on http://localhost:3030
```
